- `reda.py` is the code for the Revised Easy Data Augmentation (REDA) model, adapted from the [Easy Data Augmentation (EDA)](https://github.com/jasonwei20/eda_nlp) model. REDA is for augmenting Chinese, but it can be easily re-adapted to augment texts in other languages. Beside language difference, the REDA model improves the EDA model by automatically removing all depulicates in its outputs. Also, the REDA model does not preprocess the input text as I think it is unnecessary and does not makes sense for the basic idea of random text editing behind the EDA model. 
- `ngramLm.py` is a light n-gram language model (currently up to 4-gram but easily extendable). [Stupid backoff](https://aclanthology.org/D07-1090) is utilized in this model so that an unseen non-unigram's probability is calculated directly from the joint probability of the lower-order ngrams without discounting. The code is only written for language generation purposes, but it is possible with revision. The ngram dictionaries are available in the `data` folder.
- `redaNgramLm.py` is a combination of the REDA and Ngram language model. Specifically, the ngram LM helps the REDA to select the most linguistically likely output(s) based on the ngram dictionaries (although not neccessary true).
- `utils.py`: some helper functions for data processing and ngram generation. 
- `simnet`, standing for Similarity Networks, includes four nerual networks models (Bags of Words, Convolutional Neural Networks, and two Recurrent Neural Networks models: Long Short-Term Memory, Gated Recurrent Units). Codes are adapted from Baidu's deep learning frameworks `paddlepaddle` and `paddlenlp`. Check the folder for usage.      