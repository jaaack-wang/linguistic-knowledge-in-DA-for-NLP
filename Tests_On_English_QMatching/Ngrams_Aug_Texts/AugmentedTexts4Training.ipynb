{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2de580",
   "metadata": {},
   "source": [
    "## Text sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93136c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample, seed\n",
    "from utils import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6716c882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = load_dataset('train.txt')\n",
    "seed(175)\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cc6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_sampling(out_num):\n",
    "    return [t for t in sample(train, out_num)]\n",
    "\n",
    "\n",
    "def saveTextFile(data, filepath):\n",
    "    f = open(filepath, 'w')\n",
    "    tmp = \"\\n{}\\t{}\\t{}\"\n",
    "    first = data[0]\n",
    "    f.write(f\"{first[0]}\\t{first[1]}\\t{str(first[-1])}\")\n",
    "    for example in data[1:]:\n",
    "        f.write(tmp.format(example[0], example[1], str(example[-1])))\n",
    "    f.close()\n",
    "    print(filepath + \" has been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf33c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_k = text_sampling(10000)\n",
    "fifty_k = text_sampling(50000)\n",
    "hundred_k = text_sampling(100000)\n",
    "hund_fifty_k = text_sampling(150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7f3cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/aug_texts/train_10k.txt has been saved!\n",
      "../data/aug_texts/train_50k.txt has been saved!\n",
      "../data/aug_texts/train_100k.txt has been saved!\n",
      "../data/aug_texts/train_150k.txt has been saved!\n",
      "../data/aug_texts/train_full.txt has been saved!\n"
     ]
    }
   ],
   "source": [
    "saveTextFile(ten_k, '../data/aug_texts/train_10k.txt')\n",
    "saveTextFile(fifty_k, '../data/aug_texts/train_50k.txt')\n",
    "saveTextFile(hundred_k, '../data/aug_texts/train_100k.txt')\n",
    "saveTextFile(hund_fifty_k, '../data/aug_texts/train_150k.txt')\n",
    "saveTextFile(train, '../data/aug_texts/train_full.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f462e",
   "metadata": {},
   "source": [
    "## Text Augmentation\n",
    "\n",
    "For every text pair, we will augment both texts and do cross pairing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937d055",
   "metadata": {},
   "source": [
    "### DA models combined\n",
    "\n",
    "This is for two reasons: (1) efficiency; (2) more controlled, making sure that the augmented texts are sampled from the same pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "439301db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngramLM_en import NgramLM\n",
    "from reda_en import REDA\n",
    "from itertools import groupby\n",
    "from random import sample\n",
    "\n",
    "\n",
    "lm = NgramLM()\n",
    "\n",
    "\n",
    "class AugTextsWithTwoModels(REDA):\n",
    "    \n",
    "    def __int__(self, syn_path=None):\n",
    "        super.__init__(syn_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _out_num(edit_num, choice_num=None):\n",
    "        if choice_num:\n",
    "            return choice_num\n",
    "        if edit_num == 1:\n",
    "            return 20\n",
    "        if edit_num == 2:\n",
    "            return 50\n",
    "        if edit_num == 3:\n",
    "            return 100\n",
    "        return edit_num * 50\n",
    "    \n",
    "    @staticmethod\n",
    "    def deduplicate(ori, lst):\n",
    "        lst.append(ori)\n",
    "        lst.sort()\n",
    "        lst = [l for l,_ in groupby(lst)]\n",
    "        lst.remove(ori)\n",
    "        return lst\n",
    "    \n",
    "    def augment(self, text, replace_rate=0.2, swap_rate=0.2, \n",
    "                insert_rate=0.1, delete_rate=0.1, max_mix=None, \n",
    "                out_num_each=2, out_str=True):\n",
    "        \n",
    "        def _filter(item):\n",
    "            '''A func to make sure that the data structure is all right as some operation might fail to augment \n",
    "            the text (e.g., too short, no synonyms etc.)'''\n",
    "            if isinstance(item, str):\n",
    "                return []\n",
    "            if not out_str and isinstance(item[0], str):\n",
    "                if ''.join(item) == ''.join(words):\n",
    "                    return []\n",
    "                return [item]\n",
    "            return item\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            words = self.tokenize(text)\n",
    "        elif isinstance(text, list):\n",
    "            words = text\n",
    "        else:\n",
    "            raise TypeError(\"The input text must be either a str or a list\")\n",
    "            \n",
    "        words_num = len(words)\n",
    "        replace_num = round(replace_rate * words_num) \n",
    "        swap_num = round(swap_rate * words_num) \n",
    "        insert_num = round(insert_rate * words_num) \n",
    "        delete_num = round(delete_rate * words_num) \n",
    "        \n",
    "        reda_out = []\n",
    "        ngram_out = []\n",
    "        _sample = lambda lst, num: sample(lst, num) if len(lst) >= num else lst\n",
    "        out_num_each_special = out_num_each - 1 if out_num_each > 1 else out_num_each\n",
    "        \n",
    "        if replace_num:\n",
    "            out = _filter(self.replace_syn(words, replace_num, self._out_num(replace_num)))\n",
    "            reda_out.extend(_sample(out, out_num_each))\n",
    "            ngram_out.extend(lm.pickBestSent(out, out_num=out_num_each, out_str=out_str))\n",
    "        if swap_num:\n",
    "            out = _filter(self.swap_words(words, swap_num, self._out_num(swap_num)))\n",
    "            reda_out.extend(_sample(out, out_num_each))\n",
    "            ngram_out.extend(lm.pickBestSent(out, out_num=out_num_each, out_str=out_str))\n",
    "        if insert_num:\n",
    "            out = _filter(self.insert_words(words, insert_num, self._out_num(insert_num)))\n",
    "            reda_out.extend(_sample(out, out_num_each_special))\n",
    "            ngram_out.extend(lm.pickBestSent(out, out_num=out_num_each_special, out_str=out_str))\n",
    "        if delete_num:\n",
    "            out = _filter(self.delete_words(words, delete_num, self._out_num(delete_num)))\n",
    "            reda_out.extend(_sample(out, out_num_each_special))\n",
    "            ngram_out.extend(lm.pickBestSent(out, out_num=out_num_each_special, out_str=out_str))\n",
    "            \n",
    "        out = _filter(self.mixed_edits(words, 2, 50))\n",
    "        reda_out.extend(_sample(out, out_num_each_special))\n",
    "        ngram_out.extend(lm.pickBestSent(out, out_num=out_num_each_special, out_str=out_str))\n",
    "        \n",
    "        if out_str:\n",
    "            reda_out = [' '.join(sent) for sent in reda_out]\n",
    "        # to deduplicate the outputs and ensure that the original text is no returned.\n",
    "        words = self._out_str(words, out_str)\n",
    "        \n",
    "        reda_out = self.deduplicate(words, reda_out)\n",
    "        ngram_out = self.deduplicate(words, ngram_out)\n",
    "        return reda_out, ngram_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5fd7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textAugmentation(data, augModel, aug_num_each=2):\n",
    "    def augment(text_a, text_b, label):\n",
    "        out_reda = [(text_a, text_b, label)]\n",
    "        out_ngram = [(text_a, text_b, label)]\n",
    "        \n",
    "        aug_reda, aug_ngram = augModel.augment(text_a, out_num_each=aug_num_each)\n",
    "        out_reda.extend([(t, text_b, label) for t in aug_reda])\n",
    "        out_ngram.extend([(t, text_b, label) for t in aug_ngram])\n",
    "        \n",
    "        aug_reda, aug_ngram = augModel.augment(text_b, out_num_each=aug_num_each)\n",
    "        out_reda.extend([(text_a, t, label) for t in aug_reda])\n",
    "        out_ngram.extend([(text_a, t, label) for t in aug_ngram])\n",
    "        return out_reda, out_ngram\n",
    "    \n",
    "    if not isinstance(data[0], (tuple, list,)):\n",
    "        out_reda, out_ngram = augment(data[0], data[1], data[-1])\n",
    "        print('Texts augmented.')\n",
    "        print(f' Before (reda): 1. Now: {len(out_reda)}')\n",
    "        print(f' Before (ngram): 1. Now: {len(out_ngram)}')\n",
    "        return out_reda, out_ngram\n",
    "    \n",
    "    outputs_reda, outputs_gram = [], []\n",
    "    for example in tqdm(data):\n",
    "        out_reda, out_ngram = augment(example[0], example[1], example[-1])\n",
    "        outputs_reda.extend(out_reda)\n",
    "        outputs_gram.extend(out_ngram)\n",
    "    print('Texts augmented.')\n",
    "    print(f'Before (reda): {len(data)}. Now: {len(outputs_reda)}')\n",
    "    print(f'Before (ngram): {len(data)}. Now: {len(outputs_gram)}')\n",
    "    return outputs_reda, outputs_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "197b5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = AugTextsWithTwoModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0054c",
   "metadata": {},
   "source": [
    "### 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d0afb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 10000/10000 [13:27<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts augmented.\n",
      "Before (reda): 10000. Now: 148341\n",
      "Before (ngram): 10000. Now: 141604\n",
      "../data/aug_texts/train_10k_aug_reda.txt has been saved!\n",
      "../data/aug_texts/train_10k_aug_reda_ngram.txt has been saved!\n"
     ]
    }
   ],
   "source": [
    "ten_k_aug_reda, ten_k_aug_reda_ngram = textAugmentation(ten_k, aug)\n",
    "saveTextFile(ten_k_aug_reda, '../data/aug_texts/train_10k_aug_reda.txt')\n",
    "saveTextFile(ten_k_aug_reda_ngram, '../data/aug_texts/train_10k_aug_reda_ngram.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905bc23",
   "metadata": {},
   "source": [
    "### 50,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3b79957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 50000/50000 [7:54:09<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts augmented.\n",
      "Before (reda): 50000. Now: 543066\n",
      "Before (ngram): 50000. Now: 512176\n",
      "../data/aug_texts/train_50k_aug_reda.txt has been saved!\n",
      "../data/aug_texts/train_50k_aug_reda_ngram.txt has been saved!\n"
     ]
    }
   ],
   "source": [
    "fifty_k_aug_reda, fifty_k_aug_reda_ngram = textAugmentation(fifty_k, aug, aug_num_each=1)\n",
    "saveTextFile(fifty_k_aug_reda, '../data/aug_texts/train_50k_aug_reda.txt')\n",
    "saveTextFile(fifty_k_aug_reda_ngram, '../data/aug_texts/train_50k_aug_reda_ngram.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f9cef",
   "metadata": {},
   "source": [
    "### 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e391d7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 100000/100000 [3:09:52<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts augmented.\n",
      "Before (reda): 100000. Now: 1086063\n",
      "Before (ngram): 100000. Now: 1023777\n",
      "../data/aug_texts/train_100k_aug_reda.txt has been saved!\n",
      "../data/aug_texts/train_100k_aug_reda_ngram.txt has been saved!\n"
     ]
    }
   ],
   "source": [
    "hundred_k_aug_reda, hundred_k_aug_reda_ngram = textAugmentation(hundred_k, aug, aug_num_each=1)\n",
    "saveTextFile(hundred_k_aug_reda, '../data/aug_texts/train_100k_aug_reda.txt')\n",
    "saveTextFile(hundred_k_aug_reda_ngram, '../data/aug_texts/train_100k_aug_reda_ngram.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985e07e",
   "metadata": {},
   "source": [
    "### 150,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77e52fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 150000/150000 [4:00:35<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts augmented.\n",
      "Before (reda): 150000. Now: 1629178\n",
      "Before (ngram): 150000. Now: 1536285\n",
      "../data/aug_texts/train_150k_aug_reda.txt has been saved!\n",
      "../data/aug_texts/train_150k_aug_reda_ngram.txt has been saved!\n"
     ]
    }
   ],
   "source": [
    "hund_fifty_k_aug_reda, hund_fifty_k_aug_reda_ngram = textAugmentation(hund_fifty_k, aug, aug_num_each=1)\n",
    "saveTextFile(hund_fifty_k_aug_reda, '../data/aug_texts/train_150k_aug_reda.txt')\n",
    "saveTextFile(hund_fifty_k_aug_reda_ngram, '../data/aug_texts/train_150k_aug_reda_ngram.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cd812",
   "metadata": {},
   "source": [
    "### Full\n",
    "\n",
    "The two augmented train sets for the entire train set were gotten from another running window. The statistics for them are reported below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0f53218",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_aug_reda, full_aug_reda_ngram = textAugmentation(train, aug, aug_num_each=1)\n",
    "saveTextFile(full_aug_reda, '../data/aug_texts/train_full_aug_reda.txt')\n",
    "saveTextFile(full_aug_reda_ngram, '../data/aug_texts/train_full_aug_reda_ngram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae7e338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts augmented.\n",
      "Before (reda): 260000. Now: 2823733\n",
      "Before (ngram): 260000. Now: 2662639\n"
     ]
    }
   ],
   "source": [
    "print('''Texts augmented.\n",
    "Before (reda): 260000. Now: 2823733\n",
    "Before (ngram): 260000. Now: 2662639''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
