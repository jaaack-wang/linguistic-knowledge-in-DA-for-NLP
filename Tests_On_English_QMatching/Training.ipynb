{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T06:33:23.675351Z",
     "iopub.status.busy": "2022-01-26T06:33:23.674862Z",
     "iopub.status.idle": "2022-01-26T06:33:25.704907Z",
     "shell.execute_reply": "2022-01-26T06:33:25.704139Z",
     "shell.execute_reply.started": "2022-01-26T06:33:23.675311Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model import SimNet\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "V = TextVectorizer()\n",
    "V.load_vocab_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T06:33:30.923681Z",
     "iopub.status.busy": "2022-01-26T06:33:30.922685Z",
     "iopub.status.idle": "2022-01-26T06:33:30.933512Z",
     "shell.execute_reply": "2022-01-26T06:33:30.932843Z",
     "shell.execute_reply.started": "2022-01-26T06:33:30.923628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(model):\n",
    "    model = paddle.Model(model)\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "    parameters=model.parameters(), learning_rate=5e-4)\n",
    "    criterion = paddle.nn.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    model.prepare(optimizer, criterion, metric)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    labels, predictions = [], []\n",
    "    logits = model.predict(data_loader)\n",
    "    for batch in data_loader:\n",
    "        labels.extend(batch[-1].tolist())\n",
    "        \n",
    "    for batch in logits[0]:\n",
    "        batch = paddle.to_tensor(batch)\n",
    "        probs = F.softmax(batch, axis=1)\n",
    "        preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "        predictions.extend(preds)\n",
    "        \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "def get_accu_pre_recall_f1(preds, labels): \n",
    "    \n",
    "    tp, fp, tn, fn, right = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for (i, j) in zip(preds, labels):\n",
    "        if i == 1 and j == 1:\n",
    "            tp += 1\n",
    "            right += 1\n",
    "        elif i == 1 and j == 0:\n",
    "            fp += 1\n",
    "        elif i == 0 and j == 0:\n",
    "            tn += 1\n",
    "            right += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    \n",
    "    A = round(right/len(preds), 3)\n",
    "    \n",
    "    try:\n",
    "        P = round(tp / (tp + fp), 3)\n",
    "    except:\n",
    "        return [A, 'Nan', 'Nan', 'Nan']\n",
    "\n",
    "    R = round(tp / (tp + fn), 3)\n",
    "    F1 = round(2 * P * R / (P + R), 3)\n",
    "    \n",
    "    return [A, P, R, F1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T06:33:35.989731Z",
     "iopub.status.busy": "2022-01-26T06:33:35.989232Z",
     "iopub.status.idle": "2022-01-26T06:33:35.997604Z",
     "shell.execute_reply": "2022-01-26T06:33:35.996890Z",
     "shell.execute_reply.started": "2022-01-26T06:33:35.989695Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_train_and_evaluate(train_path, \n",
    "                          network, \n",
    "                          epochs=\"3\", \n",
    "                          dev_path='dev.txt', \n",
    "                          test_path='test.txt', \n",
    "                          batch_size=64, \n",
    "                          save=False, \n",
    "                          save_dir='ckpt',\n",
    "                          device=\"gpu\", \n",
    "                          log_freq=500):\n",
    "    \n",
    "    paddle.set_device(device)\n",
    "    \n",
    "    train_set, dev_set, test_set = load_dataset([train_path, dev_path, test_path])\n",
    "    trans_fn = get_trans_fn(V, include_seq_len=True)\n",
    "    batchify_fn = get_batchify_fn(include_seq_len=True)\n",
    "    train_loader = create_dataloader(train_set, trans_fn, batchify_fn)\n",
    "    dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn)\n",
    "    test_loader = create_dataloader(test_set, trans_fn, batchify_fn, shuffle=False)\n",
    "\n",
    "    model = SimNet(\n",
    "        network=network,\n",
    "        vocab_size=len(V),\n",
    "        num_classes=2)\n",
    "    \n",
    "    model = get_model(model)\n",
    "    if save:\n",
    "        model.fit(train_loader, dev_loader, epochs=epochs, batch_size=batch_size,\n",
    "                  verbose=2, log_freq=log_freq, save_dir=save_dir)\n",
    "    else:\n",
    "        model.fit(train_loader, dev_loader, epochs=epochs, batch_size=batch_size,\n",
    "                  verbose=2, log_freq=log_freq)\n",
    "        \n",
    "    preds, labels = predict(model, test_loader)\n",
    "    accu, prec, recall, f1 = get_accu_pre_recall_f1(preds, labels)\n",
    "    \n",
    "    return [accu, prec, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T03:55:36.871567Z",
     "iopub.status.busy": "2022-01-26T03:55:36.870970Z",
     "iopub.status.idle": "2022-01-26T04:53:29.427485Z",
     "shell.execute_reply": "2022-01-26T04:53:29.426837Z",
     "shell.execute_reply.started": "2022-01-26T03:55:36.871540Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: full; Model: gru\n",
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/3\n",
      "step  500/4063 - loss: 0.4331 - acc: 0.6655 - 29ms/step\n",
      "step 1000/4063 - loss: 0.6085 - acc: 0.6902 - 28ms/step\n",
      "step 1500/4063 - loss: 0.5816 - acc: 0.7033 - 28ms/step\n",
      "step 2000/4063 - loss: 0.4939 - acc: 0.7102 - 28ms/step\n",
      "step 2500/4063 - loss: 0.4486 - acc: 0.7158 - 28ms/step\n",
      "step 3000/4063 - loss: 0.6081 - acc: 0.7212 - 28ms/step\n",
      "step 3500/4063 - loss: 0.4941 - acc: 0.7259 - 28ms/step\n",
      "step 4000/4063 - loss: 0.5927 - acc: 0.7290 - 28ms/step\n",
      "step 4063/4063 - loss: 0.5926 - acc: 0.7295 - 28ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.5522 - acc: 0.7525 - 24ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 2/3\n",
      "step  500/4063 - loss: 0.4722 - acc: 0.7929 - 29ms/step\n",
      "step 1000/4063 - loss: 0.4321 - acc: 0.7952 - 29ms/step\n",
      "step 1500/4063 - loss: 0.4930 - acc: 0.7935 - 29ms/step\n",
      "step 2000/4063 - loss: 0.4421 - acc: 0.7933 - 29ms/step\n",
      "step 2500/4063 - loss: 0.3825 - acc: 0.7936 - 29ms/step\n",
      "step 3000/4063 - loss: 0.3945 - acc: 0.7937 - 29ms/step\n",
      "step 3500/4063 - loss: 0.4954 - acc: 0.7938 - 29ms/step\n",
      "step 4000/4063 - loss: 0.4552 - acc: 0.7942 - 29ms/step\n",
      "step 4063/4063 - loss: 0.3717 - acc: 0.7942 - 29ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.4326 - acc: 0.7674 - 24ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 3/3\n",
      "step  500/4063 - loss: 0.4436 - acc: 0.8429 - 29ms/step\n",
      "step 1000/4063 - loss: 0.4222 - acc: 0.8411 - 29ms/step\n",
      "step 1500/4063 - loss: 0.3787 - acc: 0.8391 - 29ms/step\n",
      "step 2000/4063 - loss: 0.3137 - acc: 0.8373 - 29ms/step\n",
      "step 2500/4063 - loss: 0.3456 - acc: 0.8365 - 29ms/step\n",
      "step 3000/4063 - loss: 0.4269 - acc: 0.8351 - 29ms/step\n",
      "step 3500/4063 - loss: 0.3275 - acc: 0.8349 - 29ms/step\n",
      "step 4000/4063 - loss: 0.3554 - acc: 0.8345 - 28ms/step\n",
      "step 4063/4063 - loss: 0.5138 - acc: 0.8343 - 28ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.6375 - acc: 0.7763 - 24ms/step\n",
      "Eval samples: 20000\n",
      "Predict begin...\n",
      "step 290/290 [==============================] - ETA: 17s - 61ms/st - ETA: 12s - 43ms/st - ETA: 10s - 37ms/st - ETA: 9s - 33ms/step - ETA: 8s - 31ms/ste - ETA: 8s - 30ms/ste - ETA: 7s - 29ms/ste - ETA: 7s - 28ms/ste - ETA: 7s - 28ms/ste - ETA: 7s - 27ms/ste - ETA: 7s - 27ms/ste - ETA: 7s - 27ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 23ms/ste - 23ms/step          \n",
      "Predict samples: 18526\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "out = []\n",
    "\n",
    "for size in ['10k', '50k', '100k', '150k', 'full']:\n",
    "    for net in ['bow', 'cnn', 'lstm', 'gru']:\n",
    "        print(f\"Size: {size}; Model: {net}\")\n",
    "        res = do_train_and_evaluate(f'./all_data/train_{size}.txt', net, 3, device=\"gpu\")\n",
    "        out.append(['base', 'None', net, size] + res)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        columns = ['TrainType', 'EditType', 'ClfModel', 'TrainSize', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "        df = pd.DataFrame(out, columns=columns)\n",
    "        df.to_excel('base_model_stats.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T06:33:55.950870Z",
     "iopub.status.busy": "2022-01-26T06:33:55.950340Z",
     "iopub.status.idle": "2022-01-26T23:38:00.927862Z",
     "shell.execute_reply": "2022-01-26T23:38:00.927153Z",
     "shell.execute_reply.started": "2022-01-26T06:33:55.950829Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aug_type:reda_ngram. Size: full; Model: gru\n",
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/3\n",
      "step   500/41604 - loss: 0.5444 - acc: 0.6690 - 29ms/step\n",
      "step  1000/41604 - loss: 0.5154 - acc: 0.6967 - 28ms/step\n",
      "step  1500/41604 - loss: 0.4993 - acc: 0.7104 - 28ms/step\n",
      "step  2000/41604 - loss: 0.4115 - acc: 0.7186 - 28ms/step\n",
      "step  2500/41604 - loss: 0.5137 - acc: 0.7262 - 28ms/step\n",
      "step  3000/41604 - loss: 0.4339 - acc: 0.7320 - 28ms/step\n",
      "step  3500/41604 - loss: 0.4665 - acc: 0.7370 - 28ms/step\n",
      "step  4000/41604 - loss: 0.4029 - acc: 0.7412 - 28ms/step\n",
      "step  4500/41604 - loss: 0.4107 - acc: 0.7451 - 28ms/step\n",
      "step  5000/41604 - loss: 0.5887 - acc: 0.7484 - 28ms/step\n",
      "step  5500/41604 - loss: 0.4484 - acc: 0.7515 - 28ms/step\n",
      "step  6000/41604 - loss: 0.5541 - acc: 0.7546 - 28ms/step\n",
      "step  6500/41604 - loss: 0.4489 - acc: 0.7575 - 28ms/step\n",
      "step  7000/41604 - loss: 0.3721 - acc: 0.7603 - 28ms/step\n",
      "step  7500/41604 - loss: 0.3754 - acc: 0.7629 - 28ms/step\n",
      "step  8000/41604 - loss: 0.4125 - acc: 0.7653 - 28ms/step\n",
      "step  8500/41604 - loss: 0.4181 - acc: 0.7675 - 28ms/step\n",
      "step  9000/41604 - loss: 0.4231 - acc: 0.7697 - 28ms/step\n",
      "step  9500/41604 - loss: 0.3725 - acc: 0.7719 - 28ms/step\n",
      "step 10000/41604 - loss: 0.4066 - acc: 0.7738 - 28ms/step\n",
      "step 10500/41604 - loss: 0.5531 - acc: 0.7757 - 28ms/step\n",
      "step 11000/41604 - loss: 0.3923 - acc: 0.7776 - 28ms/step\n",
      "step 11500/41604 - loss: 0.3145 - acc: 0.7796 - 28ms/step\n",
      "step 12000/41604 - loss: 0.3851 - acc: 0.7814 - 28ms/step\n",
      "step 12500/41604 - loss: 0.3828 - acc: 0.7832 - 28ms/step\n",
      "step 13000/41604 - loss: 0.3118 - acc: 0.7849 - 28ms/step\n",
      "step 13500/41604 - loss: 0.3813 - acc: 0.7867 - 28ms/step\n",
      "step 14000/41604 - loss: 0.3691 - acc: 0.7883 - 28ms/step\n",
      "step 14500/41604 - loss: 0.3569 - acc: 0.7898 - 28ms/step\n",
      "step 15000/41604 - loss: 0.3544 - acc: 0.7914 - 28ms/step\n",
      "step 15500/41604 - loss: 0.4222 - acc: 0.7931 - 28ms/step\n",
      "step 16000/41604 - loss: 0.2518 - acc: 0.7946 - 28ms/step\n",
      "step 16500/41604 - loss: 0.4151 - acc: 0.7961 - 28ms/step\n",
      "step 17000/41604 - loss: 0.2883 - acc: 0.7975 - 28ms/step\n",
      "step 17500/41604 - loss: 0.3015 - acc: 0.7991 - 28ms/step\n",
      "step 18000/41604 - loss: 0.3656 - acc: 0.8005 - 28ms/step\n",
      "step 18500/41604 - loss: 0.3177 - acc: 0.8019 - 28ms/step\n",
      "step 19000/41604 - loss: 0.2393 - acc: 0.8033 - 28ms/step\n",
      "step 19500/41604 - loss: 0.3038 - acc: 0.8046 - 28ms/step\n",
      "step 20000/41604 - loss: 0.4321 - acc: 0.8060 - 28ms/step\n",
      "step 20500/41604 - loss: 0.2156 - acc: 0.8073 - 28ms/step\n",
      "step 21000/41604 - loss: 0.3983 - acc: 0.8086 - 28ms/step\n",
      "step 21500/41604 - loss: 0.3475 - acc: 0.8098 - 28ms/step\n",
      "step 22000/41604 - loss: 0.3682 - acc: 0.8110 - 28ms/step\n",
      "step 22500/41604 - loss: 0.2844 - acc: 0.8123 - 28ms/step\n",
      "step 23000/41604 - loss: 0.2674 - acc: 0.8135 - 28ms/step\n",
      "step 23500/41604 - loss: 0.3110 - acc: 0.8147 - 28ms/step\n",
      "step 24000/41604 - loss: 0.2318 - acc: 0.8159 - 28ms/step\n",
      "step 24500/41604 - loss: 0.3715 - acc: 0.8171 - 28ms/step\n",
      "step 25000/41604 - loss: 0.2927 - acc: 0.8182 - 28ms/step\n",
      "step 25500/41604 - loss: 0.2383 - acc: 0.8193 - 28ms/step\n",
      "step 26000/41604 - loss: 0.3895 - acc: 0.8204 - 28ms/step\n",
      "step 26500/41604 - loss: 0.2277 - acc: 0.8215 - 28ms/step\n",
      "step 27000/41604 - loss: 0.3946 - acc: 0.8226 - 28ms/step\n",
      "step 27500/41604 - loss: 0.1447 - acc: 0.8236 - 28ms/step\n",
      "step 28000/41604 - loss: 0.2706 - acc: 0.8247 - 28ms/step\n",
      "step 28500/41604 - loss: 0.3216 - acc: 0.8256 - 28ms/step\n",
      "step 29000/41604 - loss: 0.2204 - acc: 0.8267 - 28ms/step\n",
      "step 29500/41604 - loss: 0.2365 - acc: 0.8277 - 28ms/step\n",
      "step 30000/41604 - loss: 0.1791 - acc: 0.8286 - 28ms/step\n",
      "step 30500/41604 - loss: 0.2746 - acc: 0.8296 - 28ms/step\n",
      "step 31000/41604 - loss: 0.1822 - acc: 0.8305 - 28ms/step\n",
      "step 31500/41604 - loss: 0.2806 - acc: 0.8314 - 28ms/step\n",
      "step 32000/41604 - loss: 0.3045 - acc: 0.8323 - 28ms/step\n",
      "step 32500/41604 - loss: 0.2623 - acc: 0.8333 - 28ms/step\n",
      "step 33000/41604 - loss: 0.4687 - acc: 0.8342 - 28ms/step\n",
      "step 33500/41604 - loss: 0.2069 - acc: 0.8351 - 28ms/step\n",
      "step 34000/41604 - loss: 0.4041 - acc: 0.8359 - 28ms/step\n",
      "step 34500/41604 - loss: 0.2385 - acc: 0.8368 - 28ms/step\n",
      "step 35000/41604 - loss: 0.1519 - acc: 0.8376 - 28ms/step\n",
      "step 35500/41604 - loss: 0.1911 - acc: 0.8385 - 28ms/step\n",
      "step 36000/41604 - loss: 0.2259 - acc: 0.8392 - 28ms/step\n",
      "step 36500/41604 - loss: 0.1370 - acc: 0.8400 - 28ms/step\n",
      "step 37000/41604 - loss: 0.1407 - acc: 0.8409 - 28ms/step\n",
      "step 37500/41604 - loss: 0.1993 - acc: 0.8417 - 28ms/step\n",
      "step 38000/41604 - loss: 0.1707 - acc: 0.8425 - 28ms/step\n",
      "step 38500/41604 - loss: 0.1869 - acc: 0.8433 - 28ms/step\n",
      "step 39000/41604 - loss: 0.2212 - acc: 0.8440 - 28ms/step\n",
      "step 39500/41604 - loss: 0.1666 - acc: 0.8448 - 28ms/step\n",
      "step 40000/41604 - loss: 0.1400 - acc: 0.8456 - 28ms/step\n",
      "step 40500/41604 - loss: 0.2840 - acc: 0.8463 - 28ms/step\n",
      "step 41000/41604 - loss: 0.2381 - acc: 0.8471 - 28ms/step\n",
      "step 41500/41604 - loss: 0.2727 - acc: 0.8478 - 28ms/step\n",
      "step 41604/41604 - loss: 0.1363 - acc: 0.8480 - 28ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.6397 - acc: 0.7902 - 23ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 2/3\n",
      "step   500/41604 - loss: 0.1789 - acc: 0.9200 - 29ms/step\n",
      "step  1000/41604 - loss: 0.2421 - acc: 0.9188 - 29ms/step\n",
      "step  1500/41604 - loss: 0.1179 - acc: 0.9203 - 29ms/step\n",
      "step  2000/41604 - loss: 0.1637 - acc: 0.9206 - 28ms/step\n",
      "step  2500/41604 - loss: 0.1766 - acc: 0.9205 - 28ms/step\n",
      "step  3000/41604 - loss: 0.1244 - acc: 0.9205 - 28ms/step\n",
      "step  3500/41604 - loss: 0.1612 - acc: 0.9203 - 28ms/step\n",
      "step  4000/41604 - loss: 0.2346 - acc: 0.9202 - 28ms/step\n",
      "step  4500/41604 - loss: 0.1369 - acc: 0.9205 - 28ms/step\n",
      "step  5000/41604 - loss: 0.1625 - acc: 0.9207 - 28ms/step\n",
      "step  5500/41604 - loss: 0.2528 - acc: 0.9211 - 28ms/step\n",
      "step  6000/41604 - loss: 0.1675 - acc: 0.9211 - 28ms/step\n",
      "step  6500/41604 - loss: 0.1523 - acc: 0.9213 - 28ms/step\n",
      "step  7000/41604 - loss: 0.2088 - acc: 0.9214 - 28ms/step\n",
      "step  7500/41604 - loss: 0.1504 - acc: 0.9215 - 28ms/step\n",
      "step  8000/41604 - loss: 0.1515 - acc: 0.9215 - 28ms/step\n",
      "step  8500/41604 - loss: 0.1241 - acc: 0.9218 - 28ms/step\n",
      "step  9000/41604 - loss: 0.1058 - acc: 0.9221 - 28ms/step\n",
      "step  9500/41604 - loss: 0.1503 - acc: 0.9222 - 28ms/step\n",
      "step 10000/41604 - loss: 0.2123 - acc: 0.9223 - 28ms/step\n",
      "step 10500/41604 - loss: 0.0977 - acc: 0.9225 - 28ms/step\n",
      "step 11000/41604 - loss: 0.1975 - acc: 0.9227 - 28ms/step\n",
      "step 11500/41604 - loss: 0.1479 - acc: 0.9228 - 28ms/step\n",
      "step 12000/41604 - loss: 0.1835 - acc: 0.9230 - 28ms/step\n",
      "step 12500/41604 - loss: 0.1778 - acc: 0.9233 - 28ms/step\n",
      "step 13000/41604 - loss: 0.2431 - acc: 0.9234 - 28ms/step\n",
      "step 13500/41604 - loss: 0.1849 - acc: 0.9237 - 28ms/step\n",
      "step 14000/41604 - loss: 0.2113 - acc: 0.9239 - 28ms/step\n",
      "step 14500/41604 - loss: 0.1825 - acc: 0.9241 - 28ms/step\n",
      "step 15000/41604 - loss: 0.2001 - acc: 0.9244 - 28ms/step\n",
      "step 15500/41604 - loss: 0.2365 - acc: 0.9245 - 28ms/step\n",
      "step 16000/41604 - loss: 0.1729 - acc: 0.9247 - 28ms/step\n",
      "step 16500/41604 - loss: 0.2003 - acc: 0.9248 - 28ms/step\n",
      "step 17000/41604 - loss: 0.1822 - acc: 0.9250 - 28ms/step\n",
      "step 17500/41604 - loss: 0.1120 - acc: 0.9251 - 28ms/step\n",
      "step 18000/41604 - loss: 0.2562 - acc: 0.9253 - 28ms/step\n",
      "step 18500/41604 - loss: 0.0798 - acc: 0.9256 - 28ms/step\n",
      "step 19000/41604 - loss: 0.1584 - acc: 0.9258 - 28ms/step\n",
      "step 19500/41604 - loss: 0.1480 - acc: 0.9259 - 28ms/step\n",
      "step 20000/41604 - loss: 0.1113 - acc: 0.9261 - 28ms/step\n",
      "step 20500/41604 - loss: 0.2585 - acc: 0.9264 - 28ms/step\n",
      "step 21000/41604 - loss: 0.2018 - acc: 0.9266 - 28ms/step\n",
      "step 21500/41604 - loss: 0.1795 - acc: 0.9268 - 28ms/step\n",
      "step 22000/41604 - loss: 0.1664 - acc: 0.9270 - 28ms/step\n",
      "step 22500/41604 - loss: 0.2197 - acc: 0.9272 - 28ms/step\n",
      "step 23000/41604 - loss: 0.1878 - acc: 0.9274 - 28ms/step\n",
      "step 23500/41604 - loss: 0.0986 - acc: 0.9277 - 28ms/step\n",
      "step 24000/41604 - loss: 0.0841 - acc: 0.9278 - 28ms/step\n",
      "step 24500/41604 - loss: 0.1989 - acc: 0.9280 - 28ms/step\n",
      "step 25000/41604 - loss: 0.1650 - acc: 0.9282 - 28ms/step\n",
      "step 25500/41604 - loss: 0.0798 - acc: 0.9284 - 28ms/step\n",
      "step 26000/41604 - loss: 0.1501 - acc: 0.9285 - 28ms/step\n",
      "step 26500/41604 - loss: 0.1101 - acc: 0.9287 - 28ms/step\n",
      "step 27000/41604 - loss: 0.1743 - acc: 0.9289 - 28ms/step\n",
      "step 27500/41604 - loss: 0.1934 - acc: 0.9291 - 28ms/step\n",
      "step 28000/41604 - loss: 0.1353 - acc: 0.9293 - 28ms/step\n",
      "step 28500/41604 - loss: 0.1027 - acc: 0.9295 - 28ms/step\n",
      "step 29000/41604 - loss: 0.0565 - acc: 0.9297 - 28ms/step\n",
      "step 29500/41604 - loss: 0.2175 - acc: 0.9299 - 28ms/step\n",
      "step 30000/41604 - loss: 0.1611 - acc: 0.9301 - 28ms/step\n",
      "step 30500/41604 - loss: 0.0751 - acc: 0.9303 - 28ms/step\n",
      "step 31000/41604 - loss: 0.1556 - acc: 0.9306 - 28ms/step\n",
      "step 31500/41604 - loss: 0.1502 - acc: 0.9308 - 28ms/step\n",
      "step 32000/41604 - loss: 0.1337 - acc: 0.9310 - 28ms/step\n",
      "step 32500/41604 - loss: 0.1713 - acc: 0.9312 - 28ms/step\n",
      "step 33000/41604 - loss: 0.0825 - acc: 0.9314 - 28ms/step\n",
      "step 33500/41604 - loss: 0.1707 - acc: 0.9316 - 28ms/step\n",
      "step 34000/41604 - loss: 0.0552 - acc: 0.9317 - 28ms/step\n",
      "step 34500/41604 - loss: 0.1461 - acc: 0.9319 - 28ms/step\n",
      "step 35000/41604 - loss: 0.1223 - acc: 0.9321 - 28ms/step\n",
      "step 35500/41604 - loss: 0.1669 - acc: 0.9322 - 28ms/step\n",
      "step 36000/41604 - loss: 0.1297 - acc: 0.9324 - 28ms/step\n",
      "step 36500/41604 - loss: 0.1690 - acc: 0.9326 - 28ms/step\n",
      "step 37000/41604 - loss: 0.0851 - acc: 0.9328 - 28ms/step\n",
      "step 37500/41604 - loss: 0.1781 - acc: 0.9330 - 28ms/step\n",
      "step 38000/41604 - loss: 0.2245 - acc: 0.9332 - 28ms/step\n",
      "step 38500/41604 - loss: 0.1008 - acc: 0.9333 - 28ms/step\n",
      "step 39000/41604 - loss: 0.1019 - acc: 0.9335 - 28ms/step\n",
      "step 39500/41604 - loss: 0.1041 - acc: 0.9337 - 28ms/step\n",
      "step 40000/41604 - loss: 0.0793 - acc: 0.9339 - 28ms/step\n",
      "step 40500/41604 - loss: 0.1412 - acc: 0.9340 - 28ms/step\n",
      "step 41000/41604 - loss: 0.1806 - acc: 0.9342 - 28ms/step\n",
      "step 41500/41604 - loss: 0.1710 - acc: 0.9344 - 28ms/step\n",
      "step 41604/41604 - loss: 0.1202 - acc: 0.9344 - 28ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.8949 - acc: 0.7864 - 23ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 3/3\n",
      "step   500/41604 - loss: 0.1310 - acc: 0.9583 - 29ms/step\n",
      "step  1000/41604 - loss: 0.2117 - acc: 0.9588 - 28ms/step\n",
      "step  1500/41604 - loss: 0.0589 - acc: 0.9589 - 28ms/step\n",
      "step  2000/41604 - loss: 0.1252 - acc: 0.9592 - 28ms/step\n",
      "step  2500/41604 - loss: 0.1256 - acc: 0.9592 - 28ms/step\n",
      "step  3000/41604 - loss: 0.2056 - acc: 0.9593 - 28ms/step\n",
      "step  3500/41604 - loss: 0.0608 - acc: 0.9592 - 28ms/step\n",
      "step  4000/41604 - loss: 0.1632 - acc: 0.9593 - 28ms/step\n",
      "step  4500/41604 - loss: 0.0931 - acc: 0.9592 - 28ms/step\n",
      "step  5000/41604 - loss: 0.1649 - acc: 0.9592 - 28ms/step\n",
      "step  5500/41604 - loss: 0.0811 - acc: 0.9593 - 28ms/step\n",
      "step  6000/41604 - loss: 0.0670 - acc: 0.9592 - 28ms/step\n",
      "step  6500/41604 - loss: 0.0686 - acc: 0.9592 - 28ms/step\n",
      "step  7000/41604 - loss: 0.0832 - acc: 0.9593 - 28ms/step\n",
      "step  7500/41604 - loss: 0.0786 - acc: 0.9593 - 28ms/step\n",
      "step  8000/41604 - loss: 0.0391 - acc: 0.9592 - 28ms/step\n",
      "step  8500/41604 - loss: 0.1414 - acc: 0.9592 - 28ms/step\n",
      "step  9000/41604 - loss: 0.1188 - acc: 0.9592 - 28ms/step\n",
      "step  9500/41604 - loss: 0.1106 - acc: 0.9592 - 28ms/step\n",
      "step 10000/41604 - loss: 0.0848 - acc: 0.9593 - 28ms/step\n",
      "step 10500/41604 - loss: 0.0371 - acc: 0.9593 - 28ms/step\n",
      "step 11000/41604 - loss: 0.1703 - acc: 0.9593 - 28ms/step\n",
      "step 11500/41604 - loss: 0.0976 - acc: 0.9593 - 28ms/step\n",
      "step 12000/41604 - loss: 0.1040 - acc: 0.9593 - 28ms/step\n",
      "step 12500/41604 - loss: 0.0810 - acc: 0.9594 - 28ms/step\n",
      "step 13000/41604 - loss: 0.1153 - acc: 0.9595 - 28ms/step\n",
      "step 13500/41604 - loss: 0.1082 - acc: 0.9595 - 28ms/step\n",
      "step 14000/41604 - loss: 0.1082 - acc: 0.9596 - 28ms/step\n",
      "step 14500/41604 - loss: 0.1241 - acc: 0.9596 - 28ms/step\n",
      "step 15000/41604 - loss: 0.1780 - acc: 0.9596 - 28ms/step\n",
      "step 15500/41604 - loss: 0.1377 - acc: 0.9596 - 28ms/step\n",
      "step 16000/41604 - loss: 0.0842 - acc: 0.9597 - 28ms/step\n",
      "step 16500/41604 - loss: 0.1529 - acc: 0.9597 - 28ms/step\n",
      "step 17000/41604 - loss: 0.1205 - acc: 0.9598 - 28ms/step\n",
      "step 17500/41604 - loss: 0.0759 - acc: 0.9598 - 28ms/step\n",
      "step 18000/41604 - loss: 0.1798 - acc: 0.9598 - 28ms/step\n",
      "step 18500/41604 - loss: 0.0282 - acc: 0.9599 - 28ms/step\n",
      "step 19000/41604 - loss: 0.0837 - acc: 0.9599 - 28ms/step\n",
      "step 19500/41604 - loss: 0.1373 - acc: 0.9599 - 28ms/step\n",
      "step 20000/41604 - loss: 0.1599 - acc: 0.9600 - 28ms/step\n",
      "step 20500/41604 - loss: 0.1191 - acc: 0.9601 - 28ms/step\n",
      "step 21000/41604 - loss: 0.0241 - acc: 0.9601 - 28ms/step\n",
      "step 21500/41604 - loss: 0.0996 - acc: 0.9601 - 28ms/step\n",
      "step 22000/41604 - loss: 0.0317 - acc: 0.9602 - 28ms/step\n",
      "step 22500/41604 - loss: 0.1215 - acc: 0.9603 - 28ms/step\n",
      "step 23000/41604 - loss: 0.0506 - acc: 0.9604 - 28ms/step\n",
      "step 23500/41604 - loss: 0.1295 - acc: 0.9604 - 28ms/step\n",
      "step 24000/41604 - loss: 0.0488 - acc: 0.9605 - 28ms/step\n",
      "step 24500/41604 - loss: 0.0663 - acc: 0.9606 - 28ms/step\n",
      "step 25000/41604 - loss: 0.0933 - acc: 0.9606 - 28ms/step\n",
      "step 25500/41604 - loss: 0.0830 - acc: 0.9607 - 28ms/step\n",
      "step 26000/41604 - loss: 0.1745 - acc: 0.9608 - 28ms/step\n",
      "step 26500/41604 - loss: 0.0760 - acc: 0.9608 - 28ms/step\n",
      "step 27000/41604 - loss: 0.0473 - acc: 0.9609 - 28ms/step\n",
      "step 27500/41604 - loss: 0.0351 - acc: 0.9609 - 28ms/step\n",
      "step 28000/41604 - loss: 0.1016 - acc: 0.9610 - 28ms/step\n",
      "step 28500/41604 - loss: 0.0588 - acc: 0.9610 - 28ms/step\n",
      "step 29000/41604 - loss: 0.1583 - acc: 0.9611 - 28ms/step\n",
      "step 29500/41604 - loss: 0.0177 - acc: 0.9612 - 28ms/step\n",
      "step 30000/41604 - loss: 0.1566 - acc: 0.9612 - 28ms/step\n",
      "step 30500/41604 - loss: 0.1156 - acc: 0.9613 - 28ms/step\n",
      "step 31000/41604 - loss: 0.0413 - acc: 0.9613 - 28ms/step\n",
      "step 31500/41604 - loss: 0.0801 - acc: 0.9614 - 28ms/step\n",
      "step 32000/41604 - loss: 0.0563 - acc: 0.9614 - 28ms/step\n",
      "step 32500/41604 - loss: 0.0274 - acc: 0.9615 - 28ms/step\n",
      "step 33000/41604 - loss: 0.1595 - acc: 0.9615 - 28ms/step\n",
      "step 33500/41604 - loss: 0.0427 - acc: 0.9616 - 28ms/step\n",
      "step 34000/41604 - loss: 0.0745 - acc: 0.9616 - 28ms/step\n",
      "step 34500/41604 - loss: 0.0954 - acc: 0.9616 - 28ms/step\n",
      "step 35000/41604 - loss: 0.0579 - acc: 0.9617 - 28ms/step\n",
      "step 35500/41604 - loss: 0.1039 - acc: 0.9618 - 28ms/step\n",
      "step 36000/41604 - loss: 0.1106 - acc: 0.9618 - 28ms/step\n",
      "step 36500/41604 - loss: 0.0296 - acc: 0.9619 - 28ms/step\n",
      "step 37000/41604 - loss: 0.0930 - acc: 0.9619 - 28ms/step\n",
      "step 37500/41604 - loss: 0.0846 - acc: 0.9620 - 28ms/step\n",
      "step 38000/41604 - loss: 0.0567 - acc: 0.9620 - 28ms/step\n",
      "step 38500/41604 - loss: 0.0776 - acc: 0.9621 - 28ms/step\n",
      "step 39000/41604 - loss: 0.1008 - acc: 0.9622 - 28ms/step\n",
      "step 39500/41604 - loss: 0.0754 - acc: 0.9622 - 28ms/step\n",
      "step 40000/41604 - loss: 0.0671 - acc: 0.9622 - 28ms/step\n",
      "step 40500/41604 - loss: 0.1008 - acc: 0.9623 - 28ms/step\n",
      "step 41000/41604 - loss: 0.0594 - acc: 0.9624 - 28ms/step\n",
      "step 41500/41604 - loss: 0.0701 - acc: 0.9624 - 28ms/step\n",
      "step 41604/41604 - loss: 0.0679 - acc: 0.9624 - 28ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.4196 - acc: 0.7856 - 23ms/step\n",
      "Eval samples: 20000\n",
      "Predict begin...\n",
      "step 290/290 [==============================] - ETA: 16s - 58ms/st - ETA: 11s - 40ms/st - ETA: 9s - 35ms/step - ETA: 8s - 31ms/ste - ETA: 8s - 30ms/ste - ETA: 7s - 28ms/ste - ETA: 7s - 27ms/ste - ETA: 7s - 27ms/ste - ETA: 7s - 26ms/ste - ETA: 7s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 24ms/ste - ETA: 6s - 24ms/ste - ETA: 6s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 23ms/ste - ETA: 4s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 3s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 2s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 1s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - ETA: 0s - 23ms/ste - 23ms/step          \n",
      "Predict samples: 18526\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "out = []\n",
    "\n",
    "for aug_type in ['reda', 'reda_ngram']:\n",
    "    for size in ['10k', '50k', '100k', '150k', 'full']:\n",
    "        for net in ['bow', 'cnn', 'lstm', 'gru']:\n",
    "            print(f\"aug_type:{aug_type}. Size: {size}; Model: {net}\")\n",
    "            res = do_train_and_evaluate(f'./all_data/train_{size}_aug_{aug_type}.txt', net, 3, device=\"gpu\")\n",
    "            out.append([aug_type, 'All', net, size] + res)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            columns = ['TrainType', 'EditType', 'ClfModel', 'TrainSize', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "            df = pd.DataFrame(out, columns=columns)\n",
    "            df.to_excel('aug_model_stats.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
