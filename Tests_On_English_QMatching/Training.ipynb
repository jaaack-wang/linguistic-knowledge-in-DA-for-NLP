{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T03:55:36.595013Z",
     "iopub.status.busy": "2022-01-26T03:55:36.594032Z",
     "iopub.status.idle": "2022-01-26T03:55:36.634734Z",
     "shell.execute_reply": "2022-01-26T03:55:36.634007Z",
     "shell.execute_reply.started": "2022-01-26T03:55:36.594966Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from model import SimNet\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "V = TextVectorizer()\n",
    "V.load_vocab_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T03:55:36.807094Z",
     "iopub.status.busy": "2022-01-26T03:55:36.806184Z",
     "iopub.status.idle": "2022-01-26T03:55:36.815968Z",
     "shell.execute_reply": "2022-01-26T03:55:36.815367Z",
     "shell.execute_reply.started": "2022-01-26T03:55:36.807058Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_model(model):\n",
    "    model = paddle.Model(model)\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "    parameters=model.parameters(), learning_rate=5e-4)\n",
    "    criterion = paddle.nn.CrossEntropyLoss()\n",
    "    metric = paddle.metric.Accuracy()\n",
    "    model.prepare(optimizer, criterion, metric)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    labels, predictions = [], []\n",
    "    logits = model.predict(data_loader)\n",
    "    for batch in data_loader:\n",
    "        labels.extend(batch[-1].tolist())\n",
    "        \n",
    "    for batch in logits[0]:\n",
    "        batch = paddle.to_tensor(batch)\n",
    "        probs = F.softmax(batch, axis=1)\n",
    "        preds = paddle.argmax(probs, axis=1).numpy().tolist()\n",
    "        predictions.extend(preds)\n",
    "        \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "def get_accu_pre_recall_f1(preds, labels): \n",
    "    \n",
    "    tp, fp, tn, fn, right = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for (i, j) in zip(preds, labels):\n",
    "        if i == 1 and j == 1:\n",
    "            tp += 1\n",
    "            right += 1\n",
    "        elif i == 1 and j == 0:\n",
    "            fp += 1\n",
    "        elif i == 0 and j == 0:\n",
    "            tn += 1\n",
    "            right += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    \n",
    "    A = round(right/len(preds), 3)\n",
    "    \n",
    "    try:\n",
    "        P = round(tp / (tp + fp), 3)\n",
    "    except:\n",
    "        return [A, 'Nan', 'Nan', 'Nan']\n",
    "\n",
    "    R = round(tp / (tp + fn), 3)\n",
    "    F1 = round(2 * P * R / (P + R), 3)\n",
    "    \n",
    "    return [A, P, R, F1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T03:55:36.817488Z",
     "iopub.status.busy": "2022-01-26T03:55:36.817238Z",
     "iopub.status.idle": "2022-01-26T03:55:36.824277Z",
     "shell.execute_reply": "2022-01-26T03:55:36.823726Z",
     "shell.execute_reply.started": "2022-01-26T03:55:36.817466Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_train_and_evaluate(train_path, \n",
    "                          network, \n",
    "                          epochs=\"3\", \n",
    "                          dev_path='dev.txt', \n",
    "                          test_path='test.txt', \n",
    "                          batch_size=64, \n",
    "                          save=False, \n",
    "                          save_dir='ckpt',\n",
    "                          device=\"gpu\", \n",
    "                          log_freq=500):\n",
    "    \n",
    "    paddle.set_device(device)\n",
    "    \n",
    "    train_set, dev_set, test_set = load_dataset([train_path, dev_path, test_path])\n",
    "    trans_fn = get_trans_fn(V, include_seq_len=True)\n",
    "    batchify_fn = get_batchify_fn(include_seq_len=True)\n",
    "    train_loader = create_dataloader(train_set, trans_fn, batchify_fn)\n",
    "    dev_loader = create_dataloader(dev_set, trans_fn, batchify_fn)\n",
    "    test_loader = create_dataloader(test_set, trans_fn, batchify_fn, shuffle=False)\n",
    "\n",
    "    model = SimNet(\n",
    "        network=network,\n",
    "        vocab_size=len(V),\n",
    "        num_classes=2)\n",
    "    \n",
    "    model = get_model(model)\n",
    "    if save:\n",
    "        model.fit(train_loader, dev_loader, epochs=epochs, batch_size=batch_size,\n",
    "                  verbose=2, log_freq=log_freq, save_dir=save_dir)\n",
    "    else:\n",
    "        model.fit(train_loader, dev_loader, epochs=epochs, batch_size=batch_size,\n",
    "                  verbose=2, log_freq=log_freq)\n",
    "        \n",
    "    preds, labels = predict(model, test_loader)\n",
    "    accu, prec, recall, f1 = get_accu_pre_recall_f1(preds, labels)\n",
    "    \n",
    "    return [accu, prec, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T03:55:36.871567Z",
     "iopub.status.busy": "2022-01-26T03:55:36.870970Z",
     "iopub.status.idle": "2022-01-26T04:53:29.427485Z",
     "shell.execute_reply": "2022-01-26T04:53:29.426837Z",
     "shell.execute_reply.started": "2022-01-26T03:55:36.871540Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: full; Model: gru\n",
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/3\n",
      "step  500/4063 - loss: 0.4331 - acc: 0.6655 - 29ms/step\n",
      "step 1000/4063 - loss: 0.6085 - acc: 0.6902 - 28ms/step\n",
      "step 1500/4063 - loss: 0.5816 - acc: 0.7033 - 28ms/step\n",
      "step 2000/4063 - loss: 0.4939 - acc: 0.7102 - 28ms/step\n",
      "step 2500/4063 - loss: 0.4486 - acc: 0.7158 - 28ms/step\n",
      "step 3000/4063 - loss: 0.6081 - acc: 0.7212 - 28ms/step\n",
      "step 3500/4063 - loss: 0.4941 - acc: 0.7259 - 28ms/step\n",
      "step 4000/4063 - loss: 0.5927 - acc: 0.7290 - 28ms/step\n",
      "step 4063/4063 - loss: 0.5926 - acc: 0.7295 - 28ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.5522 - acc: 0.7525 - 24ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 2/3\n",
      "step  500/4063 - loss: 0.4722 - acc: 0.7929 - 29ms/step\n",
      "step 1000/4063 - loss: 0.4321 - acc: 0.7952 - 29ms/step\n",
      "step 1500/4063 - loss: 0.4930 - acc: 0.7935 - 29ms/step\n",
      "step 2000/4063 - loss: 0.4421 - acc: 0.7933 - 29ms/step\n",
      "step 2500/4063 - loss: 0.3825 - acc: 0.7936 - 29ms/step\n",
      "step 3000/4063 - loss: 0.3945 - acc: 0.7937 - 29ms/step\n",
      "step 3500/4063 - loss: 0.4954 - acc: 0.7938 - 29ms/step\n",
      "step 4000/4063 - loss: 0.4552 - acc: 0.7942 - 29ms/step\n",
      "step 4063/4063 - loss: 0.3717 - acc: 0.7942 - 29ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.4326 - acc: 0.7674 - 24ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 3/3\n",
      "step  500/4063 - loss: 0.4436 - acc: 0.8429 - 29ms/step\n",
      "step 1000/4063 - loss: 0.4222 - acc: 0.8411 - 29ms/step\n",
      "step 1500/4063 - loss: 0.3787 - acc: 0.8391 - 29ms/step\n",
      "step 2000/4063 - loss: 0.3137 - acc: 0.8373 - 29ms/step\n",
      "step 2500/4063 - loss: 0.3456 - acc: 0.8365 - 29ms/step\n",
      "step 3000/4063 - loss: 0.4269 - acc: 0.8351 - 29ms/step\n",
      "step 3500/4063 - loss: 0.3275 - acc: 0.8349 - 29ms/step\n",
      "step 4000/4063 - loss: 0.3554 - acc: 0.8345 - 28ms/step\n",
      "step 4063/4063 - loss: 0.5138 - acc: 0.8343 - 28ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.6375 - acc: 0.7763 - 24ms/step\n",
      "Eval samples: 20000\n",
      "Predict begin...\n",
      "step 290/290 [==============================] - ETA: 17s - 61ms/st - ETA: 12s - 43ms/st - ETA: 10s - 37ms/st - ETA: 9s - 33ms/step - ETA: 8s - 31ms/ste - ETA: 8s - 30ms/ste - ETA: 7s - 29ms/ste - ETA: 7s - 28ms/ste - ETA: 7s - 28ms/ste - ETA: 7s - 27ms/ste - ETA: 7s - 27ms/ste - ETA: 7s - 27ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 26ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 6s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 25ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 5s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 4s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 3s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 2s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 1s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 24ms/ste - ETA: 0s - 23ms/ste - 23ms/step          \n",
      "Predict samples: 18526\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "out = []\n",
    "\n",
    "for size in ['10k', '50k', '100k', '150k', 'full']:\n",
    "    for net in ['bow', 'cnn', 'lstm', 'gru']:\n",
    "        print(f\"Size: {size}; Model: {net}\")\n",
    "        res = do_train_and_evaluate(f'./all_data/train_{size}.txt', net, 3, device=\"gpu\")\n",
    "        out.append(['base', 'None', net, size] + res)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        columns = ['TrainType', 'EditType', 'ClfModel', 'TrainSize', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "        df = pd.DataFrame(out, columns=columns)\n",
    "        df.to_excel('base_model_stats.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:53:29.444170Z",
     "iopub.status.busy": "2022-01-26T04:53:29.443793Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aug_type:reda. Size: 100k; Model: bow\n",
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/3\n",
      "step   500/16970 - loss: 0.4618 - acc: 0.6978 - 23ms/step\n",
      "step  1000/16970 - loss: 0.5146 - acc: 0.7184 - 24ms/step\n",
      "step  1500/16970 - loss: 0.3985 - acc: 0.7305 - 23ms/step\n",
      "step  2000/16970 - loss: 0.5019 - acc: 0.7397 - 24ms/step\n",
      "step  2500/16970 - loss: 0.5234 - acc: 0.7471 - 23ms/step\n",
      "step  3000/16970 - loss: 0.4164 - acc: 0.7542 - 23ms/step\n",
      "step  3500/16970 - loss: 0.5033 - acc: 0.7604 - 23ms/step\n",
      "step  4000/16970 - loss: 0.4378 - acc: 0.7657 - 23ms/step\n",
      "step  4500/16970 - loss: 0.4711 - acc: 0.7703 - 23ms/step\n",
      "step  5000/16970 - loss: 0.3753 - acc: 0.7750 - 23ms/step\n",
      "step  5500/16970 - loss: 0.3408 - acc: 0.7794 - 23ms/step\n",
      "step  6000/16970 - loss: 0.4934 - acc: 0.7834 - 23ms/step\n",
      "step  6500/16970 - loss: 0.3445 - acc: 0.7877 - 23ms/step\n",
      "step  7000/16970 - loss: 0.2212 - acc: 0.7916 - 23ms/step\n",
      "step  7500/16970 - loss: 0.3010 - acc: 0.7954 - 23ms/step\n",
      "step  8000/16970 - loss: 0.3767 - acc: 0.7989 - 23ms/step\n",
      "step  8500/16970 - loss: 0.2025 - acc: 0.8022 - 23ms/step\n",
      "step  9000/16970 - loss: 0.3339 - acc: 0.8055 - 23ms/step\n",
      "step  9500/16970 - loss: 0.2666 - acc: 0.8087 - 23ms/step\n",
      "step 10000/16970 - loss: 0.2398 - acc: 0.8116 - 23ms/step\n",
      "step 10500/16970 - loss: 0.2397 - acc: 0.8146 - 23ms/step\n",
      "step 11000/16970 - loss: 0.2433 - acc: 0.8175 - 23ms/step\n",
      "step 11500/16970 - loss: 0.2733 - acc: 0.8202 - 23ms/step\n",
      "step 12000/16970 - loss: 0.3408 - acc: 0.8230 - 23ms/step\n",
      "step 12500/16970 - loss: 0.1889 - acc: 0.8256 - 23ms/step\n",
      "step 13000/16970 - loss: 0.1084 - acc: 0.8282 - 23ms/step\n",
      "step 13500/16970 - loss: 0.3021 - acc: 0.8307 - 23ms/step\n",
      "step 14000/16970 - loss: 0.2552 - acc: 0.8332 - 23ms/step\n",
      "step 14500/16970 - loss: 0.3344 - acc: 0.8355 - 23ms/step\n",
      "step 15000/16970 - loss: 0.2762 - acc: 0.8378 - 23ms/step\n",
      "step 15500/16970 - loss: 0.2412 - acc: 0.8401 - 23ms/step\n",
      "step 16000/16970 - loss: 0.2130 - acc: 0.8423 - 23ms/step\n",
      "step 16500/16970 - loss: 0.2181 - acc: 0.8444 - 23ms/step\n",
      "step 16970/16970 - loss: 0.1785 - acc: 0.8465 - 23ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 0.7566 - acc: 0.7157 - 21ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 2/3\n",
      "step   500/16970 - loss: 0.1365 - acc: 0.9321 - 24ms/step\n",
      "step  1000/16970 - loss: 0.1773 - acc: 0.9301 - 24ms/step\n",
      "step  1500/16970 - loss: 0.2120 - acc: 0.9302 - 24ms/step\n",
      "step  2000/16970 - loss: 0.1654 - acc: 0.9314 - 23ms/step\n",
      "step  2500/16970 - loss: 0.2636 - acc: 0.9312 - 23ms/step\n",
      "step  3000/16970 - loss: 0.1853 - acc: 0.9318 - 23ms/step\n",
      "step  3500/16970 - loss: 0.1615 - acc: 0.9321 - 23ms/step\n",
      "step  4000/16970 - loss: 0.3030 - acc: 0.9329 - 23ms/step\n",
      "step  4500/16970 - loss: 0.1068 - acc: 0.9336 - 23ms/step\n",
      "step  5000/16970 - loss: 0.2202 - acc: 0.9342 - 23ms/step\n",
      "step  5500/16970 - loss: 0.0948 - acc: 0.9349 - 23ms/step\n",
      "step  6000/16970 - loss: 0.1457 - acc: 0.9353 - 23ms/step\n",
      "step  6500/16970 - loss: 0.0448 - acc: 0.9359 - 24ms/step\n",
      "step  7000/16970 - loss: 0.0617 - acc: 0.9367 - 24ms/step\n",
      "step  7500/16970 - loss: 0.1084 - acc: 0.9374 - 24ms/step\n",
      "step  8000/16970 - loss: 0.1103 - acc: 0.9381 - 24ms/step\n",
      "step  8500/16970 - loss: 0.1000 - acc: 0.9388 - 24ms/step\n",
      "step  9000/16970 - loss: 0.1456 - acc: 0.9394 - 24ms/step\n",
      "step  9500/16970 - loss: 0.0880 - acc: 0.9400 - 24ms/step\n",
      "step 10000/16970 - loss: 0.1061 - acc: 0.9405 - 24ms/step\n",
      "step 10500/16970 - loss: 0.0984 - acc: 0.9412 - 24ms/step\n",
      "step 11000/16970 - loss: 0.0890 - acc: 0.9418 - 24ms/step\n",
      "step 11500/16970 - loss: 0.0980 - acc: 0.9424 - 24ms/step\n",
      "step 12000/16970 - loss: 0.1069 - acc: 0.9429 - 24ms/step\n",
      "step 12500/16970 - loss: 0.0916 - acc: 0.9435 - 24ms/step\n",
      "step 13000/16970 - loss: 0.1112 - acc: 0.9441 - 24ms/step\n",
      "step 13500/16970 - loss: 0.0271 - acc: 0.9446 - 24ms/step\n",
      "step 14000/16970 - loss: 0.0593 - acc: 0.9451 - 24ms/step\n",
      "step 14500/16970 - loss: 0.1255 - acc: 0.9457 - 24ms/step\n",
      "step 15000/16970 - loss: 0.0979 - acc: 0.9462 - 24ms/step\n",
      "step 15500/16970 - loss: 0.0654 - acc: 0.9467 - 24ms/step\n",
      "step 16000/16970 - loss: 0.0430 - acc: 0.9473 - 24ms/step\n",
      "step 16500/16970 - loss: 0.2018 - acc: 0.9477 - 24ms/step\n",
      "step 16970/16970 - loss: 0.0455 - acc: 0.9482 - 24ms/step\n",
      "Eval begin...\n",
      "step 313/313 - loss: 1.0115 - acc: 0.7203 - 21ms/step\n",
      "Eval samples: 20000\n",
      "Epoch 3/3\n",
      "step   500/16970 - loss: 0.0530 - acc: 0.9746 - 23ms/step\n",
      "step  1000/16970 - loss: 0.0919 - acc: 0.9734 - 23ms/step\n",
      "step  1500/16970 - loss: 0.0327 - acc: 0.9745 - 24ms/step\n",
      "step  2000/16970 - loss: 0.0500 - acc: 0.9748 - 23ms/step\n",
      "step  2500/16970 - loss: 0.0650 - acc: 0.9747 - 23ms/step\n",
      "step  3000/16970 - loss: 0.1040 - acc: 0.9746 - 23ms/step\n",
      "step  3500/16970 - loss: 0.0342 - acc: 0.9747 - 24ms/step\n",
      "step  4000/16970 - loss: 0.0468 - acc: 0.9748 - 24ms/step\n",
      "step  4500/16970 - loss: 0.0455 - acc: 0.9747 - 24ms/step\n",
      "step  5000/16970 - loss: 0.0535 - acc: 0.9748 - 24ms/step\n",
      "step  5500/16970 - loss: 0.0372 - acc: 0.9748 - 24ms/step\n",
      "step  6000/16970 - loss: 0.0300 - acc: 0.9750 - 24ms/step\n",
      "step  6500/16970 - loss: 0.0625 - acc: 0.9751 - 24ms/step\n",
      "step  7000/16970 - loss: 0.0492 - acc: 0.9752 - 24ms/step\n",
      "step  7500/16970 - loss: 0.0501 - acc: 0.9753 - 24ms/step\n",
      "step  8000/16970 - loss: 0.0710 - acc: 0.9754 - 24ms/step\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "out = []\n",
    "\n",
    "for aug_type in ['reda', 'reda_ngram']:\n",
    "    for size in ['10k', '50k', '100k', '150k', 'full']:\n",
    "        for net in ['bow', 'cnn', 'lstm', 'gru']:\n",
    "            print(f\"aug_type:{aug_type}. Size: {size}; Model: {net}\")\n",
    "            res = do_train_and_evaluate(f'./all_data/train_{size}_aug_{aug_type}.txt', net, 3, device=\"gpu\")\n",
    "            out.append([aug_type, 'All', net, size] + res)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            columns = ['TrainType', 'EditType', 'ClfModel', 'TrainSize', 'Accuracy', 'Precision', 'Recall', 'F1']\n",
    "            df = pd.DataFrame(out, columns=columns)\n",
    "            df.to_excel('aug_model_stats.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
